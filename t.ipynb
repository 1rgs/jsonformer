{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jsonllm.logits_processors.NumberStoppingCriteria at 0x1777c4d60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jsonllm.logits_processors import NumberStoppingCriteria, OutputNumbersTokens\n",
    "\n",
    "NumberStoppingCriteria(tokenizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JSONLLM:\n",
    "    value: Dict[str, Any] = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        json_schema: Dict[str, Any],\n",
    "        prompt: str,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.json_schema = json_schema\n",
    "        self.prompt = prompt\n",
    "\n",
    "        self.number_logit_processor = OutputNumbersTokens(self.tokenizer, self.prompt)\n",
    "        self.number_stop_criteria = NumberStoppingCriteria(self.tokenizer, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_number(self, suffix=\"\") -> float:\n",
    "        print(\"generate_number\", suffix)\n",
    "        prompt = self.get_prompt() + suffix\n",
    "\n",
    "        print(\"\\033[91m {}\\033[00m\".format(prompt))\n",
    "        response = self.model.generate(\n",
    "            self.tokenizer.encode(prompt, return_tensors=\"pt\"),\n",
    "            max_new_tokens=6,\n",
    "            num_return_sequences=1,\n",
    "            logits_processor=[self.number_logit_processor],\n",
    "            stopping_criteria=[self.number_stop_criteria],\n",
    "            temperature=1.5,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        response = self.tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "        print(\"response is\")\n",
    "        print(\"\\033[94m {}\\033[00m\".format(response))\n",
    "        response = response.strip().rstrip(\".\").lstrip(\"0\")\n",
    "        try:\n",
    "            return float(response)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            return \n",
    "\n",
    "    def generate_boolean(self, suffix=\"\") -> bool:\n",
    "        prompt = self.get_prompt()\n",
    "        true_token_id = self.tokenizer.encode(\"true\", add_special_tokens=False)[0]\n",
    "        false_token_id = self.tokenizer.encode(\"false\", add_special_tokens=False)[0]\n",
    "\n",
    "        response = self.generate(\n",
    "            prompt, forced_bos_token_id=[true_token_id, false_token_id]\n",
    "        ).lower()\n",
    "\n",
    "        if response == \"true\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def generate_array(\n",
    "        self, item_schema: Dict[str, Any], obj: Dict[str, Any], suffix=\"\"\n",
    "    ) -> list:\n",
    "        array_length = random.randint(0, 5)\n",
    "        return [self.generate_value(item_schema, obj) for _ in range(array_length)]\n",
    "\n",
    "    # add stopping criteria with \"\n",
    "    def generate_string(self) -> str:\n",
    "        prompt = self.get_prompt()\n",
    "        response = self.generate(prompt)\n",
    "        return response\n",
    "\n",
    "    def generate_object(\n",
    "        self, properties: Dict[str, Any], obj: Dict[str, Any], suffix=\"\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        print(\"generate_object\", properties)\n",
    "\n",
    "        for key, schema in properties.items():\n",
    "            value = self.generate_value(schema, obj, suffix=f'\"{key}\": ')\n",
    "\n",
    "            obj[key] = value\n",
    "        return obj\n",
    "\n",
    "    def generate_value(self, schema: Dict[str, Any], obj: Dict[str, Any], suffix=\"\"):\n",
    "        schema_type = schema[\"type\"]\n",
    "        if schema_type == \"number\":\n",
    "            return self.generate_number(suffix=suffix)\n",
    "        elif schema_type == \"boolean\":\n",
    "            return self.generate_boolean(suffix=suffix)\n",
    "        elif schema_type == \"array\":\n",
    "            return self.generate_array(schema[\"items\"], obj, suffix=suffix)\n",
    "        elif schema_type == \"object\":\n",
    "            return self.generate_object(schema[\"properties\"], obj, suffix=suffix)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported schema type: {schema_type}\")\n",
    "\n",
    "    def get_prompt(self):\n",
    "        template = \"\"\"{prompt}\\nMake sure to output in the following format:\\n{schema}\\n Result: {progress}\"\"\"\n",
    "        progress = json.dumps(self.value)\n",
    "\n",
    "        progress = progress.rstrip(\"}\").rstrip(\"]\").rstrip(\",\")\n",
    "\n",
    "        prompt = template.format(\n",
    "            prompt=self.prompt,\n",
    "            schema=json.dumps(self.json_schema),\n",
    "            progress=progress,\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def __call__(self) -> Dict[str, Any]:\n",
    "        self.value = {}\n",
    "        generated_data = self.generate_object(\n",
    "            self.json_schema[\"properties\"], self.value\n",
    "        )\n",
    "        return generated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_object {'temperature': {'type': 'number'}}\n",
      "generate_number \"temperature\": \n",
      "\u001b[91m Generate a weather object\n",
      "Make sure to output in the following format:\n",
      "{\"type\": \"object\", \"properties\": {\"temperature\": {\"type\": \"number\"}}}\n",
      " Result: {\"temperature\": \u001b[00m\n",
      "Stopping because of multiple .\n",
      "response is\n",
      "\u001b[94m Generate a weather object\n",
      "Make sure to output in the following format:\n",
      "{\"type\": \"object\", \"properties\": {\"temperature\": {\"type\": \"number\"}}}\n",
      " Result: {\"temperature\": 000000000.0.\u001b[00m\n",
      "ValueError\n",
      "{'temperature': None}\n"
     ]
    }
   ],
   "source": [
    "weather_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"temperature\": {\"type\": \"number\"},\n",
    "        # \"humidity\": {\"type\": \"number\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "jsonllm = JSONLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=weather_schema,\n",
    "    prompt=\"Generate a weather object\",\n",
    ")\n",
    "\n",
    "output = jsonllm()\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsonllm-4DT72Dd8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

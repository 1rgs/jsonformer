{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2579,  292, 7568]])\n",
      " 28\n"
     ]
    }
   ],
   "source": [
    "to_tokenize = \" 28asdf\"\n",
    "input_ids = tokenizer.encode(to_tokenize, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "\n",
    "to_decode = [2579]\n",
    "print(tokenizer.decode(to_decode, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    LogitsWarper,StoppingCriteria\n",
    ")\n",
    "\n",
    "\n",
    "# class NumberStoppingCriteria(StoppingCriteria):\n",
    "#     \"\"\"\n",
    "#     This class can be used to stop generation when there is a repeated decimal point in the generated text.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "#         decoded = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#         if \"..\" in decoded:\n",
    "#             return True\n",
    "        \n",
    "#         if ' ' in decoded.strip():\n",
    "#             return True\n",
    "        \n",
    "#         return False\n",
    "\n",
    "\n",
    "class OutputNumbersTokens(LogitsWarper):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, prompt: str):\n",
    "        self.whitelist_tokens = [tokenizer.eos_token_id]\n",
    "        self.tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        for token_str, token_id in tokenizer.get_vocab().items():\n",
    "\n",
    "            if (\n",
    "                token_str.startswith(\"Ä \") and (\n",
    "                    all(c.isdigit() or c == \".\" for c in token_str[1:]) and token_str.count(\".\") <= 1\n",
    "                ) or (\n",
    "                    all(c.isdigit() or c == \".\" for c in token_str) and token_str.count(\".\") <= 1\n",
    "                )\n",
    "            ):\n",
    "                self.whitelist_tokens.append(token_id)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        input_ids = input_ids[:, len(self.tokenized_prompt[\"input_ids\"][0]):]\n",
    "        scores[\n",
    "            :, [i for i in range(len(scores[0])) if i not in self.whitelist_tokens]\n",
    "        ] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "a = OutputNumbersTokens(tokenizer, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found space28 2579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = OutputNumbersTokens(tokenizer, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65 tensor(-3.7568)\n",
      " 60 tensor(-3.8070)\n",
      " 75 tensor(-3.9135)\n",
      " 80 tensor(-4.0881)\n",
      " 87 tensor(-4.1663)\n",
      " 82 tensor(-4.2518)\n",
      " 50 tensor(-4.2757)\n",
      " 5 tensor(-4.3758)\n",
      " 64 tensor(-4.3760)\n",
      " 72 tensor(-4.4309)\n",
      "tensor([[ 1820, 49890,   338,  2479,   287,   812,   318,  6135]])\n",
      "my grandma's age in years is 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 tensor(-3.4328)\n",
      " 5 tensor(-4.0209)\n",
      " 36 tensor(-4.2786)\n",
      " 19 tensor(-4.4433)\n",
      " 59 tensor(-4.5570)\n",
      " 24 tensor(-4.7431)\n",
      " 33 tensor(-4.7621)\n",
      " 56 tensor(-4.7745)\n",
      " 47 tensor(-4.8474)\n",
      " 53 tensor(-4.8719)\n",
      "tensor([[40838,   338,  5951,   287,  7370,   269,  1424,   318,   657]])\n",
      "today's temperature in degrees cels is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 tensor(-5.0538)\n",
      " 5 tensor(-6.4346)\n",
      " 7 tensor(-7.2013)\n",
      " 60 tensor(-7.4283)\n",
      " 24 tensor(-7.6246)\n",
      " 42 tensor(-7.6409)\n",
      " 50 tensor(-7.6574)\n",
      " 30 tensor(-7.6641)\n",
      " 18 tensor(-7.7838)\n",
      " 59 tensor(-7.8880)\n",
      "tensor([[40838,   338,  5951,   287,   277,   318,   657]])\n",
      "today's temperature in f is 0\n",
      " 0 tensor(-1.6487)\n",
      " 5 tensor(-3.0234)\n",
      " 7 tensor(-4.6462)\n",
      " 16 tensor(-5.9200)\n",
      " 18 tensor(-6.3192)\n",
      " 19 tensor(-6.6764)\n",
      " 24 tensor(-6.7219)\n",
      " 30 tensor(-7.0326)\n",
      "2 tensor(-7.3029)\n",
      " 50 tensor(-7.7194)\n",
      "tensor([[  16, 1343,  352,  796,  657]])\n",
      "1 + 1 = 0\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"my grandma's age in years is\", \"today's temperature in degrees cels is\", \"today's temperature in f is\", \n",
    "           \"1 + 1 =\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    number = model.generate(\n",
    "        tokenizer.encode(prompt, return_tensors=\"pt\"),\n",
    "        max_new_tokens=5,\n",
    "        num_return_sequences=1,\n",
    "        logits_processor=[a],\n",
    "        stopping_criteria=[NumberStoppingCriteria(tokenizer)],\n",
    "    )   \n",
    "\n",
    "    print(number)\n",
    "\n",
    "    decoded_output = tokenizer.decode(number[0], skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_output.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsonllm-4DT72Dd8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
